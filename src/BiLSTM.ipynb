{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cules/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import random\n",
    "from random import randint\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import io\n",
    "import collections\n",
    "import math\n",
    "import argparse\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetData import getData\n",
    "data_rows,sents,addict_label=getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12772 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sents)\n",
    "sequences = tokenizer.texts_to_sequences(sents)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pad_sequences(sequences, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(addict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = np.asarray(labels)\n",
    "#SHUFFLING\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "labels = labels[indices]\n",
    "MAX_SEQUENCE_LENGTH = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cules/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Addicted', 'Addiction-prone', 'NA', 'Recovered', 'Recovering'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "ft_embed = FastText.load('./../models/first_eval_dataset_fasttext.model')\n",
    "w2v_embed = Word2Vec.load('./../models/first_eval_dataset_word2vec.model')\n",
    "# f = open('glove.twitter.27B.100d.txt','r')\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cules/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=150\n",
    "not_found=[]\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for (word, i) in word_index.items():\n",
    "    embedding_vector = None\n",
    "    try:\n",
    "        embedding_vector = ft_embed[word]\n",
    "    except:\n",
    "        embbedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        not_found.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0] # The paddings in the sequence correspond to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'a',\n",
       " 'my',\n",
       " 'if',\n",
       " 'iâ€™m',\n",
       " 't',\n",
       " 'iâ€™d',\n",
       " 'd',\n",
       " 'm',\n",
       " 'â€œi',\n",
       " '3d',\n",
       " '3g',\n",
       " 'â€œmy',\n",
       " 'â€œno',\n",
       " '29',\n",
       " 'btw',\n",
       " '1g',\n",
       " 'uk',\n",
       " '4mmc',\n",
       " 'â€œiâ€™m',\n",
       " '5htp',\n",
       " 'n',\n",
       " '2g',\n",
       " '99',\n",
       " '67',\n",
       " 'nsfw',\n",
       " 'mr',\n",
       " '44',\n",
       " '39',\n",
       " 'nyc',\n",
       " '65',\n",
       " 'â€œdo',\n",
       " 'Î¼g',\n",
       " 'pbj',\n",
       " '48',\n",
       " '72',\n",
       " 'eh',\n",
       " 'uv',\n",
       " 'iq',\n",
       " 'hbwr',\n",
       " '46',\n",
       " '31',\n",
       " '92',\n",
       " 'fbi',\n",
       " '79',\n",
       " 'o',\n",
       " 'gf',\n",
       " 'y',\n",
       " 's',\n",
       " 'uh',\n",
       " '66',\n",
       " '47',\n",
       " 'ua',\n",
       " '77',\n",
       " 'â€œa',\n",
       " '5h',\n",
       " 'ii',\n",
       " '52',\n",
       " '38',\n",
       " '4d',\n",
       " 'xr',\n",
       " '8a',\n",
       " 'w',\n",
       " 'mcg',\n",
       " 'ðŸ“·',\n",
       " 'â™¥',\n",
       " 'Âµg',\n",
       " 'dj',\n",
       " 'iÕšm',\n",
       " 'fs',\n",
       " 'uhh',\n",
       " 'fb',\n",
       " 'gta5',\n",
       " '5x3',\n",
       " 'km',\n",
       " '2k',\n",
       " 'dvd',\n",
       " 'â€œyehâ€',\n",
       " 'lmfao',\n",
       " 'Â·',\n",
       " 'cpr',\n",
       " 'ipa',\n",
       " 'bmw',\n",
       " 'i5',\n",
       " 'v',\n",
       " 'ðŸ˜',\n",
       " 'ðŸ™ŒðŸ¼',\n",
       " '1k',\n",
       " '43',\n",
       " 'ssdi',\n",
       " 'ðŸ’•',\n",
       " 'ðŸŽ¶',\n",
       " 'ðŸ”¥ðŸ”¥',\n",
       " 'ðŸŽ‰',\n",
       " 'k2',\n",
       " 'xo',\n",
       " 'iii',\n",
       " '08',\n",
       " 'gpa',\n",
       " '7991',\n",
       " 'â€˜my',\n",
       " '9k',\n",
       " '7686',\n",
       " 'xyz',\n",
       " '3s',\n",
       " '7s',\n",
       " 'zu',\n",
       " '6apdb',\n",
       " '97',\n",
       " '49',\n",
       " 'cdc',\n",
       " 'tp',\n",
       " 'q4',\n",
       " 'mmj',\n",
       " 'l',\n",
       " 'ðŸ˜‰',\n",
       " 'nba',\n",
       " '78',\n",
       " '09',\n",
       " '6â€™2â€',\n",
       " 'bp',\n",
       " 't10h',\n",
       " '6abp',\n",
       " '8b',\n",
       " 't7',\n",
       " 'ltr',\n",
       " 't3',\n",
       " 't4',\n",
       " '57',\n",
       " '5x',\n",
       " '56',\n",
       " '94',\n",
       " 't0055',\n",
       " '9lb',\n",
       " 'gvg',\n",
       " 'uo',\n",
       " '943',\n",
       " '947',\n",
       " 'â€œuh',\n",
       " 'â€œwe',\n",
       " 'â€œwhyâ€',\n",
       " '89',\n",
       " '4x',\n",
       " '8g',\n",
       " '48h',\n",
       " '2c',\n",
       " 'jr',\n",
       " '66kg',\n",
       " 'ooohhh',\n",
       " 'ðŸ˜…',\n",
       " 'ðŸ‹',\n",
       " '7g',\n",
       " 'â€œ3dâ€',\n",
       " '2cx',\n",
       " '025g',\n",
       " 'mhm',\n",
       " 'â€œmeâ€',\n",
       " '0031',\n",
       " 'mxe',\n",
       " 'xs',\n",
       " 'â€œif',\n",
       " '46g',\n",
       " '03',\n",
       " '07',\n",
       " 'bw3s',\n",
       " 'âœŒï¸ï¸',\n",
       " '2pk',\n",
       " 'â¤ï¸ðŸ™',\n",
       " '88']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found # words not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration count 1\n",
      "Epoch 1/9\n",
      "487/487 [==============================] - 107s 219ms/step - loss: 0.4739 - acc: 0.7984\n",
      "Epoch 2/9\n",
      "487/487 [==============================] - 71s 146ms/step - loss: 0.4541 - acc: 0.7979\n",
      "Epoch 3/9\n",
      "487/487 [==============================] - 80s 165ms/step - loss: 0.4408 - acc: 0.8041\n",
      "Epoch 4/9\n",
      "487/487 [==============================] - 77s 159ms/step - loss: 0.4236 - acc: 0.8094\n",
      "Epoch 5/9\n",
      "487/487 [==============================] - 81s 167ms/step - loss: 0.4101 - acc: 0.8197\n",
      "Epoch 6/9\n",
      "487/487 [==============================] - 104s 214ms/step - loss: 0.4029 - acc: 0.8230\n",
      "Epoch 7/9\n",
      "487/487 [==============================] - 74s 152ms/step - loss: 0.3945 - acc: 0.8251\n",
      "Epoch 8/9\n",
      "448/487 [==========================>...] - ETA: 6s - loss: 0.3912 - acc: 0.8272 "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "seed = 21\n",
    "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "f_cvscores = []\n",
    "prec_cvscores = []\n",
    "recall_cvscores = []\n",
    "iteration_count=1\n",
    "print(\"Iteration count\",iteration_count)\n",
    "iteration_count+=1\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "model.add(LSTM(180,dropout=0.25, recurrent_dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_labels_train = to_categorical(np.asarray(y_train))\n",
    "model_labels_test = to_categorical(np.asarray(y_test))\n",
    "model.fit(X_train, model_labels_train, epochs=9)\n",
    "y_pred=model.predict(X_test)\n",
    "pred = np.zeros(y_pred.shape)\n",
    "for i in range(y_pred.shape[0]):\n",
    "    ind = np.argmax(y_pred[i])\n",
    "    pred[i][ind] = 1\n",
    "\n",
    "\n",
    "f_cvscores.append(precision_recall_fscore_support(model_labels_test,pred,average='weighted')[2])\n",
    "prec_cvscores.append(precision_recall_fscore_support(model_labels_test,pred,average='weighted')[0])\n",
    "recall_cvscores.append(precision_recall_fscore_support(model_labels_test,pred,average='weighted')[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(prec_cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(recall_cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f_cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iteration count', 1)\n",
      "Epoch 1/23\n",
      "1079/1079 [==============================] - 12s 12ms/step - loss: 0.6457 - acc: 0.6209\n",
      "Epoch 2/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5961 - acc: 0.6784\n",
      "Epoch 3/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5519 - acc: 0.7183\n",
      "Epoch 4/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5050 - acc: 0.7590\n",
      "Epoch 5/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4812 - acc: 0.7776\n",
      "Epoch 6/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4804 - acc: 0.7563\n",
      "Epoch 7/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4424 - acc: 0.7998\n",
      "Epoch 8/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4538 - acc: 0.7804\n",
      "Epoch 9/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4247 - acc: 0.8072\n",
      "Epoch 10/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4144 - acc: 0.8017\n",
      "Epoch 11/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3891 - acc: 0.8221\n",
      "Epoch 12/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3859 - acc: 0.8452A: 0s - loss: 0.3686 \n",
      "Epoch 13/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3659 - acc: 0.8452\n",
      "Epoch 14/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3529 - acc: 0.8536\n",
      "Epoch 15/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3318 - acc: 0.8582\n",
      "Epoch 16/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3158 - acc: 0.8628\n",
      "Epoch 17/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3065 - acc: 0.8619\n",
      "Epoch 18/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3012 - acc: 0.8832\n",
      "Epoch 19/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2660 - acc: 0.8906A: 1s - loss: 0.2759 \n",
      "Epoch 20/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2479 - acc: 0.8999\n",
      "Epoch 21/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2449 - acc: 0.8971\n",
      "Epoch 22/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2409 - acc: 0.8971\n",
      "Epoch 23/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2251 - acc: 0.9147\n",
      "('Iteration count', 2)\n",
      "Epoch 1/23\n",
      "1079/1079 [==============================] - 13s 12ms/step - loss: 0.6667 - acc: 0.5644\n",
      "Epoch 2/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5917 - acc: 0.6867\n",
      "Epoch 3/23\n",
      "1079/1079 [==============================] - ETA: 0s - loss: 0.5497 - acc: 0.710 - 1s 1ms/step - loss: 0.5507 - acc: 0.7108\n",
      "Epoch 4/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5017 - acc: 0.7433\n",
      "Epoch 5/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4652 - acc: 0.7859\n",
      "Epoch 6/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4768 - acc: 0.7665\n",
      "Epoch 7/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4405 - acc: 0.7924\n",
      "Epoch 8/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4221 - acc: 0.8072\n",
      "Epoch 9/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4025 - acc: 0.8276\n",
      "Epoch 10/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4121 - acc: 0.8119\n",
      "Epoch 11/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3881 - acc: 0.8332\n",
      "Epoch 12/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3693 - acc: 0.8387\n",
      "Epoch 13/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3488 - acc: 0.8563\n",
      "Epoch 14/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3316 - acc: 0.8582\n",
      "Epoch 15/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3106 - acc: 0.8740\n",
      "Epoch 16/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3144 - acc: 0.8740\n",
      "Epoch 17/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2896 - acc: 0.8814\n",
      "Epoch 18/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2771 - acc: 0.8953\n",
      "Epoch 19/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2899 - acc: 0.8693\n",
      "Epoch 20/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2638 - acc: 0.8962\n",
      "Epoch 21/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2664 - acc: 0.8971\n",
      "Epoch 22/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2387 - acc: 0.9212\n",
      "Epoch 23/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2022 - acc: 0.9296\n",
      "('Iteration count', 3)\n",
      "Epoch 1/23\n",
      "1079/1079 [==============================] - 13s 12ms/step - loss: 0.6555 - acc: 0.6061\n",
      "Epoch 2/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5986 - acc: 0.6830\n",
      "Epoch 3/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5477 - acc: 0.7238\n",
      "Epoch 4/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5121 - acc: 0.7405\n",
      "Epoch 5/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4891 - acc: 0.7646\n",
      "Epoch 6/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4784 - acc: 0.7720\n",
      "Epoch 7/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4500 - acc: 0.7943\n",
      "Epoch 8/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4316 - acc: 0.7989\n",
      "Epoch 9/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4254 - acc: 0.7970\n",
      "Epoch 10/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3948 - acc: 0.8239\n",
      "Epoch 11/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3735 - acc: 0.8313\n",
      "Epoch 12/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3585 - acc: 0.8397\n",
      "Epoch 13/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3525 - acc: 0.8424\n",
      "Epoch 14/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3376 - acc: 0.8638\n",
      "Epoch 15/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3154 - acc: 0.8740\n",
      "Epoch 16/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3190 - acc: 0.8619\n",
      "Epoch 17/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2957 - acc: 0.8786\n",
      "Epoch 18/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2640 - acc: 0.8999\n",
      "Epoch 19/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2561 - acc: 0.9064A: 1s - loss: 0.242\n",
      "Epoch 20/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2511 - acc: 0.8962\n",
      "Epoch 21/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2311 - acc: 0.9045\n",
      "Epoch 22/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2090 - acc: 0.9157\n",
      "Epoch 23/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2107 - acc: 0.9175\n",
      "('Iteration count', 4)\n",
      "Epoch 1/23\n",
      "1079/1079 [==============================] - 13s 12ms/step - loss: 0.6519 - acc: 0.5913\n",
      "Epoch 2/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5836 - acc: 0.6914\n",
      "Epoch 3/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5389 - acc: 0.7220\n",
      "Epoch 4/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.5153 - acc: 0.7386\n",
      "Epoch 5/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4702 - acc: 0.7766\n",
      "Epoch 6/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4532 - acc: 0.7859\n",
      "Epoch 7/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4286 - acc: 0.8082\n",
      "Epoch 8/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4189 - acc: 0.8174\n",
      "Epoch 9/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.4021 - acc: 0.8285\n",
      "Epoch 10/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3935 - acc: 0.8378\n",
      "Epoch 11/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3647 - acc: 0.8462\n",
      "Epoch 12/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3656 - acc: 0.8434\n",
      "Epoch 13/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3527 - acc: 0.8526\n",
      "Epoch 14/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3181 - acc: 0.8656\n",
      "Epoch 15/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3016 - acc: 0.8703A: 0s - loss: 0.2928 - \n",
      "Epoch 16/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3032 - acc: 0.8758\n",
      "Epoch 17/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.3007 - acc: 0.8767\n",
      "Epoch 18/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2744 - acc: 0.8897A: 0s - loss: 0.2808 - a\n",
      "Epoch 19/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2758 - acc: 0.8888\n",
      "Epoch 20/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2465 - acc: 0.9045\n",
      "Epoch 21/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2214 - acc: 0.9231\n",
      "Epoch 22/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2186 - acc: 0.9212\n",
      "Epoch 23/23\n",
      "1079/1079 [==============================] - 1s 1ms/step - loss: 0.2006 - acc: 0.9231\n",
      "('Iteration count', 5)\n",
      "Epoch 1/23\n",
      "1080/1080 [==============================] - 13s 12ms/step - loss: 0.6526 - acc: 0.6065\n",
      "Epoch 2/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.6112 - acc: 0.6556\n",
      "Epoch 3/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.5534 - acc: 0.7157\n",
      "Epoch 4/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.5140 - acc: 0.7472\n",
      "Epoch 5/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4901 - acc: 0.7556\n",
      "Epoch 6/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4711 - acc: 0.7630\n",
      "Epoch 7/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4558 - acc: 0.7769\n",
      "Epoch 8/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4426 - acc: 0.7963\n",
      "Epoch 9/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4237 - acc: 0.7981\n",
      "Epoch 10/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3935 - acc: 0.8241\n",
      "Epoch 11/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3937 - acc: 0.8120\n",
      "Epoch 12/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3402 - acc: 0.8528\n",
      "Epoch 13/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3595 - acc: 0.8500\n",
      "Epoch 14/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3433 - acc: 0.8537\n",
      "Epoch 15/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3327 - acc: 0.8574\n",
      "Epoch 16/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2968 - acc: 0.8806\n",
      "Epoch 17/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2799 - acc: 0.8833\n",
      "Epoch 18/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2750 - acc: 0.8880\n",
      "Epoch 19/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2459 - acc: 0.9065\n",
      "Epoch 20/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2485 - acc: 0.9000\n",
      "Epoch 21/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2171 - acc: 0.9157\n",
      "Epoch 22/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2058 - acc: 0.9102\n",
      "Epoch 23/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2038 - acc: 0.9278\n",
      "('Iteration count', 6)\n",
      "Epoch 1/23\n",
      "1080/1080 [==============================] - 13s 12ms/step - loss: 0.6698 - acc: 0.5648\n",
      "Epoch 2/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.5987 - acc: 0.6667\n",
      "Epoch 3/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.5505 - acc: 0.7074\n",
      "Epoch 4/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.5136 - acc: 0.7537\n",
      "Epoch 5/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4807 - acc: 0.7685\n",
      "Epoch 6/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4544 - acc: 0.7880\n",
      "Epoch 7/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4354 - acc: 0.8019\n",
      "Epoch 8/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4234 - acc: 0.8065\n",
      "Epoch 9/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3975 - acc: 0.8185\n",
      "Epoch 10/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.4024 - acc: 0.8361\n",
      "Epoch 11/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3790 - acc: 0.8370\n",
      "Epoch 12/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3480 - acc: 0.8574\n",
      "Epoch 13/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3286 - acc: 0.8620\n",
      "Epoch 14/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3548 - acc: 0.8398\n",
      "Epoch 15/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3307 - acc: 0.8574\n",
      "Epoch 16/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.3004 - acc: 0.8787\n",
      "Epoch 17/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2812 - acc: 0.8907\n",
      "Epoch 18/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2735 - acc: 0.8870\n",
      "Epoch 19/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2700 - acc: 0.8926\n",
      "Epoch 20/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2323 - acc: 0.9093\n",
      "Epoch 21/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2265 - acc: 0.9120\n",
      "Epoch 22/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2214 - acc: 0.9213\n",
      "Epoch 23/23\n",
      "1080/1080 [==============================] - 1s 1ms/step - loss: 0.2039 - acc: 0.9287\n",
      "('Iteration count', 7)\n",
      "Epoch 1/23\n",
      "1081/1081 [==============================] - 13s 12ms/step - loss: 0.6574 - acc: 0.5976\n",
      "Epoch 2/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5975 - acc: 0.6809\n",
      "Epoch 3/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5393 - acc: 0.7225\n",
      "Epoch 4/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5055 - acc: 0.7586\n",
      "Epoch 5/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4981 - acc: 0.7660\n",
      "Epoch 6/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4678 - acc: 0.7780\n",
      "Epoch 7/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4512 - acc: 0.7965\n",
      "Epoch 8/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4187 - acc: 0.8067\n",
      "Epoch 9/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4002 - acc: 0.8122\n",
      "Epoch 10/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4014 - acc: 0.8298\n",
      "Epoch 11/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3897 - acc: 0.8168\n",
      "Epoch 12/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3602 - acc: 0.8464\n",
      "Epoch 13/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3346 - acc: 0.8649\n",
      "Epoch 14/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3390 - acc: 0.8520\n",
      "Epoch 15/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3122 - acc: 0.8816\n",
      "Epoch 16/23\n",
      "1081/1081 [==============================] - 1s 986us/step - loss: 0.3131 - acc: 0.8668\n",
      "Epoch 17/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3014 - acc: 0.8733\n",
      "Epoch 18/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2593 - acc: 0.8945\n",
      "Epoch 19/23\n",
      "1081/1081 [==============================] - 2s 2ms/step - loss: 0.2709 - acc: 0.8908\n",
      "Epoch 20/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2567 - acc: 0.9029\n",
      "Epoch 21/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2016 - acc: 0.9306\n",
      "Epoch 22/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2089 - acc: 0.9204A: 1s - loss: 0.1913 \n",
      "Epoch 23/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2159 - acc: 0.9195\n",
      "('Iteration count', 8)\n",
      "Epoch 1/23\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 0.6559 - acc: 0.6078\n",
      "Epoch 2/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5800 - acc: 0.6827\n",
      "Epoch 3/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5418 - acc: 0.7206\n",
      "Epoch 4/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5050 - acc: 0.7623\n",
      "Epoch 5/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4686 - acc: 0.7780\n",
      "Epoch 6/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4550 - acc: 0.7771\n",
      "Epoch 7/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4316 - acc: 0.7919\n",
      "Epoch 8/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4211 - acc: 0.8057\n",
      "Epoch 9/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4058 - acc: 0.8178\n",
      "Epoch 10/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3878 - acc: 0.8316\n",
      "Epoch 11/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3670 - acc: 0.8372A: 0s - loss: 0.3463 - acc: \n",
      "Epoch 12/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3653 - acc: 0.8437\n",
      "Epoch 13/23\n",
      "1081/1081 [==============================] - 1s 927us/step - loss: 0.3527 - acc: 0.8492\n",
      "Epoch 14/23\n",
      "1081/1081 [==============================] - 1s 874us/step - loss: 0.3277 - acc: 0.8575\n",
      "Epoch 15/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3056 - acc: 0.8659\n",
      "Epoch 16/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3233 - acc: 0.8501\n",
      "Epoch 17/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2970 - acc: 0.8696\n",
      "Epoch 18/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2815 - acc: 0.8853\n",
      "Epoch 19/23\n",
      "1081/1081 [==============================] - 1s 940us/step - loss: 0.2599 - acc: 0.8945\n",
      "Epoch 20/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2621 - acc: 0.8871\n",
      "Epoch 21/23\n",
      "1081/1081 [==============================] - 1s 902us/step - loss: 0.2419 - acc: 0.9001\n",
      "Epoch 22/23\n",
      "1081/1081 [==============================] - 1s 873us/step - loss: 0.2241 - acc: 0.9103\n",
      "Epoch 23/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.1954 - acc: 0.9251\n",
      "('Iteration count', 9)\n",
      "Epoch 1/23\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 0.6476 - acc: 0.6078\n",
      "Epoch 2/23\n",
      "1081/1081 [==============================] - 2s 2ms/step - loss: 0.5884 - acc: 0.6864\n",
      "Epoch 3/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5311 - acc: 0.7401\n",
      "Epoch 4/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5006 - acc: 0.7539\n",
      "Epoch 5/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4739 - acc: 0.7724\n",
      "Epoch 6/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4694 - acc: 0.7752\n",
      "Epoch 7/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4355 - acc: 0.7974\n",
      "Epoch 8/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4405 - acc: 0.7928\n",
      "Epoch 9/23\n",
      "1081/1081 [==============================] - 2s 2ms/step - loss: 0.4120 - acc: 0.8002\n",
      "Epoch 10/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3956 - acc: 0.8168\n",
      "Epoch 11/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3757 - acc: 0.8279\n",
      "Epoch 12/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3566 - acc: 0.8437\n",
      "Epoch 13/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3543 - acc: 0.8520\n",
      "Epoch 14/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3382 - acc: 0.8594\n",
      "Epoch 15/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3133 - acc: 0.8733\n",
      "Epoch 16/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3019 - acc: 0.8659\n",
      "Epoch 17/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2757 - acc: 0.8853\n",
      "Epoch 18/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2893 - acc: 0.8760\n",
      "Epoch 19/23\n",
      "1081/1081 [==============================] - 2s 1ms/step - loss: 0.2734 - acc: 0.8918\n",
      "Epoch 20/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2476 - acc: 0.9019\n",
      "Epoch 21/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2448 - acc: 0.8992\n",
      "Epoch 22/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2435 - acc: 0.9112\n",
      "Epoch 23/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2156 - acc: 0.9186\n",
      "('Iteration count', 10)\n",
      "Epoch 1/23\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 0.6540 - acc: 0.6142\n",
      "Epoch 2/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5904 - acc: 0.6781\n",
      "Epoch 3/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5450 - acc: 0.7391\n",
      "Epoch 4/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5064 - acc: 0.7493\n",
      "Epoch 5/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4832 - acc: 0.7687\n",
      "Epoch 6/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4711 - acc: 0.7808\n",
      "Epoch 7/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4375 - acc: 0.8057\n",
      "Epoch 8/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4065 - acc: 0.8076\n",
      "Epoch 9/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4093 - acc: 0.8187\n",
      "Epoch 10/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3925 - acc: 0.8187\n",
      "Epoch 11/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3750 - acc: 0.8400\n",
      "Epoch 12/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3554 - acc: 0.8464\n",
      "Epoch 13/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3280 - acc: 0.8594\n",
      "Epoch 14/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3249 - acc: 0.8575\n",
      "Epoch 15/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3351 - acc: 0.8631\n",
      "Epoch 16/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.3107 - acc: 0.8686\n",
      "Epoch 17/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2868 - acc: 0.8834\n",
      "Epoch 18/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2719 - acc: 0.8853\n",
      "Epoch 19/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2431 - acc: 0.9056\n",
      "Epoch 20/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2477 - acc: 0.8955\n",
      "Epoch 21/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2284 - acc: 0.9066\n",
      "Epoch 22/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2287 - acc: 0.9149\n",
      "Epoch 23/23\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2048 - acc: 0.9334\n"
     ]
    }
   ],
   "source": [
    "#Bi-LSTM Implementation\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed = 21\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "f_cvscores = []\n",
    "prec_cvscores = []\n",
    "recall_cvscores = []\n",
    "iteration_count=1\n",
    "for train_ind, test_ind in kfold.split(X, labels):\n",
    "    print(\"Iteration count\",iteration_count)\n",
    "    iteration_count+=1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False))\n",
    "    model.add(Bidirectional(LSTM(46,dropout=0.22, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_labels = to_categorical(np.asarray(labels))\n",
    "    model.fit(X[train_ind], model_labels[train_ind], epochs=23)\n",
    "    y_pred=model.predict(X[test_ind])\n",
    "    pred=[]\n",
    "    for pair in y_pred:\n",
    "        if pair[0]>pair[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    test=[]\n",
    "    for pair in model_labels[test_ind]:\n",
    "        if pair[0]>pair[1]:\n",
    "            test.append(0)\n",
    "        else:\n",
    "            test.append(1)\n",
    "    f_cvscores.append(precision_recall_fscore_support(test,pred,average='weighted')[2])\n",
    "    prec_cvscores.append(precision_recall_fscore_support(test,pred,average='weighted')[0])\n",
    "    recall_cvscores.append(precision_recall_fscore_support(test,pred,average='weighted')[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7605219418073418"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prec_cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7557708868671436"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(recall_cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553751974437712"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f_cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
