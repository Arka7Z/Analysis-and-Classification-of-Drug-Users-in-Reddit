{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import random\n",
    "from random import randint\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import io\n",
    "import collections\n",
    "import math\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetData import getData\n",
    "\n",
    "data_rows,X,Y=getData()\n",
    "unlabelled_data,_,_=getData(filename='modified_REDDITORSINRECOVERY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabelled_data['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright (C) 2010 Mathieu Blondel\n",
    "#\n",
    "# This program is free software; you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation; either version 2 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License along\n",
    "# with this program; if not, write to the Free Software Foundation, Inc.,\n",
    "# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n",
    "#\n",
    "# Implementation details are described at\n",
    "# http://www.mblondel.org/journal/2010/06/21/semi-supervised-naive-bayes-in-python/\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Implementation of Naive Bayes trained by EM for semi-supervised text\n",
    "classification, as described in\n",
    "\n",
    "\"Semi-Supervised Text Classification Using EM\", by Nigam et al.\n",
    "\n",
    "Notation:\n",
    "\n",
    "    w: word\n",
    "    d: document\n",
    "    c: class\n",
    "\n",
    "    V: vocabulary size\n",
    "    X: number of documents\n",
    "    M: number of classes\n",
    "\"\"\"\n",
    "\n",
    "def softmax(loga, k=-np.inf, out=None):\n",
    "    \"\"\"\n",
    "    Compute the sotfmax function (normalized exponentials) without underflow.\n",
    "\n",
    "    exp^a_i / \\sum_j exp^a_j\n",
    "    \"\"\"\n",
    "    if out is None: out = np.empty_like(loga).astype(np.double)\n",
    "    m = np.max(loga)\n",
    "    logam = loga - m\n",
    "    sup = logam > k\n",
    "    inf = np.logical_not(sup)\n",
    "    out[sup] = np.exp(logam[sup])\n",
    "    out[inf] = 0.0\n",
    "    out /= np.sum(out)\n",
    "    return out\n",
    "\n",
    "def logsum(loga, k=-np.inf):\n",
    "    \"\"\"\n",
    "    Compute a sum of logs without underflow.\n",
    "\n",
    "    \\log \\sum_c e^{b_c} = log [(\\sum_c e^{b_c}) e^{-B}e^B]\n",
    "                        = log [(\\sum_c e^{b_c-B}) e^B]\n",
    "                        = [log(\\sum_c e^{b_c-B}) + B\n",
    "\n",
    "    where B = max_c b_c\n",
    "    \"\"\"\n",
    "    B = np.max(loga)\n",
    "    logaB = aB = loga - B\n",
    "    sup = logaB > k\n",
    "    inf = np.logical_not(sup)\n",
    "    aB[sup] = np.exp(logaB[sup])\n",
    "    aB[inf] = 0.0\n",
    "    return (np.log(np.sum(aB)) + B)\n",
    "\n",
    "def loglikelihood(td, delta, tdu, p_w_c_log, p_c_log):\n",
    "    V, Xl = td.shape\n",
    "    V_, Xu = tdu.shape\n",
    "    Xl_, M = delta.shape\n",
    "\n",
    "    lik = 0.0\n",
    "\n",
    "    ## labeled\n",
    "    # log P(x|c)\n",
    "    p_x_c_log = np.zeros((Xl,M), np.double)\n",
    "    for w,d in zip(*td.nonzero()):\n",
    "        p_x_c_log[d,:] += p_w_c_log[w,:] * td[w,d]\n",
    "\n",
    "    # add log P(c) + lop P(x|c) if x has label c\n",
    "    for d,c in zip(*delta.nonzero()):\n",
    "        lik += p_c_log[c] + p_x_c_log[d,c]\n",
    "\n",
    "    ## unlabelled\n",
    "    # log P(x|c)\n",
    "    p_x_c_log = np.zeros((Xu,M), np.double)\n",
    "    for w,d in zip(*tdu.nonzero()):\n",
    "        p_x_c_log[d,:] += p_w_c_log[w,:] * tdu[w,d]\n",
    "\n",
    "    # add log P(c)\n",
    "    p_x_c_log += p_c_log[np.newaxis,:]\n",
    "\n",
    "    for d in range(Xu):\n",
    "        lik += logsum(p_x_c_log[d,:], k=-10)\n",
    "\n",
    "    return lik\n",
    "\n",
    "def normalize_p_c(p_c):\n",
    "    M = len(p_c)\n",
    "    denom = M + np.sum(p_c)\n",
    "    p_c += 1.0\n",
    "    p_c /= denom\n",
    "\n",
    "def normalize_p_w_c(p_w_c):\n",
    "    V, X = p_w_c.shape\n",
    "    denoms = V + np.sum(p_w_c, axis=0)\n",
    "    p_w_c += 1.0\n",
    "    p_w_c /= denoms[np.newaxis,:]\n",
    "\n",
    "class SemiNB(object):\n",
    "\n",
    "    def __init__(self, model=None):\n",
    "        \"\"\"\n",
    "        model: a model, as returned by get_model() or train().\n",
    "        \"\"\"\n",
    "        self.p_w_c = None\n",
    "        self.p_c = None\n",
    "        if model is not None: self.set_model(model)\n",
    "        self.debug = False\n",
    "\n",
    "    def train(self, td, delta, normalize=True, sparse=True):\n",
    "        \"\"\"\n",
    "        td: term-document matrix V x X\n",
    "        delta: X x M matrix\n",
    "               where delta(d,c) = 1 if document d belongs to class c\n",
    "        \"\"\"\n",
    "\n",
    "        X_, M = delta.shape\n",
    "        V, X = td.shape\n",
    "        assert(X_ == X)\n",
    "\n",
    "        # P(c)\n",
    "        self.p_c = np.sum(delta, axis=0)\n",
    "\n",
    "        # P(w|c)\n",
    "        self.p_w_c = np.zeros((V,M), dtype=np.double)\n",
    "\n",
    "        if sparse:\n",
    "            # faster when delta is sparse\n",
    "            # select indices of documents that have class c\n",
    "            for d,c in zip(*delta.nonzero()):\n",
    "                # select indices of terms that are non-zero\n",
    "                for w in np.flatnonzero(td[:,d]):\n",
    "                    self.p_w_c[w,c] += td[w,d] * delta[d,c]\n",
    "        else:\n",
    "            # faster when delta is non-sparse\n",
    "            for w,d in zip(*td.nonzero()):\n",
    "                self.p_w_c[w,:] += td[w,d] * delta[d,:]\n",
    "\n",
    "        if normalize:\n",
    "            normalize_p_c(self.p_c)\n",
    "            normalize_p_w_c(self.p_w_c)\n",
    "\n",
    "        return self.get_model()\n",
    "\n",
    "    def train_semi(self, td, delta, tdu, maxiter=50, eps=0.01):\n",
    "        \"\"\"\n",
    "        td: V x X term document matrix\n",
    "        delta: X x M label matrix\n",
    "        tdu: V x Xu term document matrix (unlabeled)\n",
    "        maxiter: maximum number of iterations\n",
    "        eps: stop if no more progress than esp\n",
    "        \"\"\"\n",
    "        X_, M = delta.shape\n",
    "        V, X = td.shape\n",
    "        assert(X_ == X)\n",
    "\n",
    "        # compute counts for labeled data once for all\n",
    "        self.train(td, delta, normalize=False)\n",
    "        p_c_l = np.array(self.p_c, copy=True)\n",
    "        p_w_c_l = np.array(self.p_w_c, copy=True)\n",
    "\n",
    "        # normalize to get initial classifier\n",
    "        normalize_p_c(self.p_c)\n",
    "        normalize_p_w_c(self.p_w_c)\n",
    "\n",
    "        lik = loglikelihood(td, delta, tdu, np.log(self.p_w_c), np.log(self.p_c))\n",
    "\n",
    "        for iteration in range(1, maxiter+1):\n",
    "            # E-step: find the probabilistic labels for unlabeled data\n",
    "            delta_u = self.predict_proba_all(tdu)\n",
    "\n",
    "            # M-step: train classifier with the union of\n",
    "            #         labeled and unlabeled data\n",
    "            self.train(tdu, delta_u, normalize=False, sparse=False)\n",
    "            self.p_c += p_c_l\n",
    "            self.p_w_c += p_w_c_l\n",
    "            normalize_p_c(self.p_c)\n",
    "            normalize_p_w_c(self.p_w_c)\n",
    "\n",
    "            lik_new = loglikelihood(td, delta, tdu,\n",
    "                                    np.log(self.p_w_c), np.log(self.p_c))\n",
    "            lik_diff = lik_new - lik\n",
    "            #assert(lik_diff >= -1e-10)\n",
    "            lik = lik_new\n",
    "\n",
    "            if lik_diff < eps:\n",
    "                print (\"No more progress, stopping EM at iteration\", iteration)\n",
    "                break\n",
    "\n",
    "            if self.debug:\n",
    "                print (\"Iteration\", iteration)\n",
    "                print (\"L += %f\" % lik_diff)\n",
    "\n",
    "        return self.get_model()\n",
    "\n",
    "    def p_x_c_log_all(self, td):\n",
    "        M = len(self.p_c)\n",
    "        V, X = td.shape\n",
    "        p_x_c_log = np.zeros((X,M), np.double)\n",
    "        p_w_c_log = np.log(self.p_w_c)\n",
    "\n",
    "        # log P(x|c)\n",
    "        for w,d in zip(*td.nonzero()):\n",
    "            p_x_c_log[d,:] += p_w_c_log[w,:] * td[w,d]\n",
    "\n",
    "        return p_x_c_log\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        x: a V array representing a document\n",
    "\n",
    "        Compute a M array containing P(c|x).\n",
    "        \"\"\"\n",
    "        return self.predict_proba_all(x[:,np.newaxis])[0,:]\n",
    "\n",
    "    def predict_proba_all(self, td):\n",
    "        \"\"\"\n",
    "        td: V x X term document matrix\n",
    "\n",
    "        Compute an X x M matrix of P(c|x) for all x in td.\n",
    "        \"\"\"\n",
    "        V, X = td.shape\n",
    "\n",
    "        # log P(x|c)\n",
    "        p_x_c_log = self.p_x_c_log_all(td)\n",
    "\n",
    "        # add log P(c)\n",
    "        p_x_c_log += np.log(self.p_c)[np.newaxis,:]\n",
    "\n",
    "        # sotfmax(log P(x|c) + log P(c)) = P(c|x)\n",
    "        for d in range(X):\n",
    "            softmax(p_x_c_log[d,:], k=-10, out=p_x_c_log[d,:])\n",
    "\n",
    "        return p_x_c_log\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        x: a V array representing a document\n",
    "\n",
    "        Compute the predicted class index.\n",
    "        \"\"\"\n",
    "        return self.predict_all(x[:,np.newaxis])[0]\n",
    "\n",
    "    def predict_all(self, td):\n",
    "        \"\"\"\n",
    "        td: V x X term document matrix\n",
    "\n",
    "        Compute a X array containing predicted class indices.\n",
    "\n",
    "        Note: the main difference with predict_proba_all is that the\n",
    "              normalization is not necessary, as we are only interested in the most\n",
    "              likely class.\n",
    "        \"\"\"\n",
    "\n",
    "        # log P(x|c)\n",
    "        p_x_c_log = self.p_x_c_log_all(td)\n",
    "\n",
    "        # add log P(c)\n",
    "        p_x_c_log += np.log(self.p_c)[np.newaxis,:]\n",
    "\n",
    "        return p_x_c_log.argmax(axis=1)\n",
    "\n",
    "    def get_model(self):\n",
    "        return (self.p_w_c, self.p_c)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.p_w_c, self.p_c = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Tf-Idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='utf-8', ngram_range=(1, 2), stop_words='english')\n",
    "def getFeatures(X):\n",
    "    features = tfidf.fit_transform(X).toarray() \n",
    "    return features\n",
    "features=getFeatures(X)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "total_data=data_rows['body']+unlabelled_data['body']\n",
    "X_total=vec.fit_transform(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_total[:627]\n",
    "X_test=X_total[627:727]\n",
    "X_unlab=X_total[727:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=(X_train.transpose()).toarray()\n",
    "X_test=(X_test.transpose()).toarray()\n",
    "X_unlab=(X_unlab.transpose()).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi=SemiNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 1, 0, 0, 4, 3, 3, 1, 0, 4, 0,\n",
       "       0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0,\n",
       "       0, 0, 3, 4, 4, 3, 3, 3, 0, 3, 4, 4, 0, 2, 2, 0, 0, 2, 3, 0, 0, 0,\n",
       "       2, 2, 0, 0, 2, 2, 2, 0, 2, 3, 0, 0, 2, 3, 3, 1, 2, 4, 2, 0, 2, 0,\n",
       "       0, 0, 2, 1, 4, 0, 4, 0, 0, 4, 2, 4, 0, 0, 0, 0, 0, 0, 0, 4, 2, 3,\n",
       "       4, 0, 2, 0, 1, 4, 2, 2, 1, 1, 2, 0, 2, 1, 2, 2, 0, 2, 4, 2, 1, 4,\n",
       "       0, 2, 4, 4, 1, 1, 0, 0, 1, 1, 2, 1, 0, 3, 3, 0, 3, 3, 2, 2, 4, 2,\n",
       "       3, 0, 3, 4, 3, 2, 0, 3, 2, 4, 3, 2, 4, 4, 4, 2, 2, 3, 3, 2, 2, 2,\n",
       "       0, 3, 2, 3, 3, 3, 3, 4, 3, 4, 2, 3, 4, 4, 4, 4, 0, 1, 2, 2, 2, 2,\n",
       "       0, 2, 4, 0, 2, 2, 2, 2, 2, 0, 0, 0, 3, 4, 4, 3, 2, 4, 4, 4, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 3, 4,\n",
       "       4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4, 2, 3, 3, 3, 2, 3, 4,\n",
       "       3, 3, 3, 3, 3, 3, 4, 4, 3, 2, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 3, 3,\n",
       "       3, 4, 3, 4, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3,\n",
       "       4, 4, 3, 3, 2, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 2, 3, 3, 4, 3, 3,\n",
       "       3, 3, 4, 3, 4, 4, 3, 3, 2, 3, 3, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, 4,\n",
       "       2, 3, 2, 2, 0, 2, 1, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 4,\n",
       "       2, 0, 0, 0, 0, 2, 0, 0, 2, 4, 0, 2, 0, 1, 1, 0, 0, 0, 1, 1, 2, 2,\n",
       "       0, 2, 0, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 1, 1, 2, 0, 2, 2, 0, 2, 0,\n",
       "       0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0, 2, 0,\n",
       "       1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2,\n",
       "       2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "       0, 2, 2, 2, 0, 2, 2, 0, 4, 0, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2,\n",
       "       0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 3, 3,\n",
       "       3, 4, 4, 4, 3, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 0, 4, 3, 4,\n",
       "       3, 4, 4, 4, 4, 3, 3, 2, 0, 4, 3, 3, 4, 4, 3, 4, 3, 4, 3, 3, 3, 0,\n",
       "       4, 2, 3, 3, 2, 3, 3, 0, 4, 4, 3, 0, 4, 4, 4, 4, 4, 4, 2, 3, 3, 4,\n",
       "       3, 4, 4, 4, 4, 4, 0, 4, 3, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 3, 0, 3,\n",
       "       3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.zeros(shape=(X_train.shape[1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,_ in enumerate(data_rows['body'][:627]):\n",
    "    delta[i][labels[i]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.68563000e-05, 4.50897286e-05, 2.70401817e-05, 2.84940875e-05,\n",
       "         3.52273928e-05],\n",
       "        [3.37126001e-05, 4.50897286e-05, 2.70401817e-05, 2.84940875e-05,\n",
       "         3.52273928e-05],\n",
       "        [2.52844501e-05, 4.50897286e-05, 2.70401817e-05, 2.84940875e-05,\n",
       "         3.52273928e-05],\n",
       "        ...,\n",
       "        [2.52844501e-05, 4.50897286e-05, 2.70401817e-05, 2.84940875e-05,\n",
       "         3.52273928e-05],\n",
       "        [8.42815002e-06, 4.50897286e-05, 8.11205451e-05, 2.84940875e-05,\n",
       "         3.52273928e-05],\n",
       "        [2.52844501e-05, 4.50897286e-05, 2.70401817e-05, 2.84940875e-05,\n",
       "         3.52273928e-05]]),\n",
       " array([0.41455696, 0.05379747, 0.20411392, 0.19936709, 0.12816456]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi.train(td=X_train,delta=delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi.train_semi(td=X_train,delta=delta,tdu=X_unlab,maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=semi.predict_all(td=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=labels[627:727]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "prec,recall,fscore,_=precision_recall_fscore_support(y_test,y_pred,average='weighted')\n",
    "print(\"Precision: \",prec,\" Recall: \",recall,\" F-score: \",fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
